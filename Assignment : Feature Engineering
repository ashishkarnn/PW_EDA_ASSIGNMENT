{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1vzT3YN+oTF2FzUYTpsdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkarnn/PW_EDA_ASSIGNMENT/blob/main/Assignment%20%3A%20Feature%20Engineering\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTRPb4R_LbQL"
      },
      "outputs": [],
      "source": [
        "In machine learning, a **parameter** is a variable that is learned from the training data during the model training process. These are the internal variables of a model that are adjusted to minimize the error or loss function.\n",
        "\n",
        "For example:\n",
        "\n",
        "- In linear regression, the **parameters** are the coefficients (weights) and the intercept of the line.\n",
        "- In neural networks, the parameters are the weights and biases in each layer.\n",
        "\n",
        "The goal of training is to find the optimal values for these parameters that allow the model to make accurate predictions or classifications on new, unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xr4z6NEQL6TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is correlation? What does negative correlation mean?\n",
        "**Correlation** is a statistical measure that describes the strength and direction of a relationship between two variables. It quantifies how much one variable changes when the other variable changes.\n",
        "\n",
        "- **Positive Correlation**: If two variables increase or decrease together, they have a positive correlation. For example, the more hours you study, the higher your exam scores might be. The correlation coefficient (r) in this case is between 0 and +1.\n",
        "\n",
        "- **Negative Correlation**: If one variable increases while the other decreases, they have a negative correlation. For example, as the temperature increases, the number of people wearing heavy jackets decreases. In this case, the correlation coefficient is between 0 and -1.\n",
        "\n",
        "A **correlation coefficient** is a number between -1 and 1 that indicates the strength of the correlation:\n",
        "- A value closer to +1 indicates a strong positive correlation.\n",
        "- A value closer to -1 indicates a strong negative correlation.\n",
        "- A value closer to 0 indicates little to no correlation.\n",
        "\n",
        "So, **negative correlation** specifically means that when one variable goes up, the other tends to go down, and vice versa.\n"
      ],
      "metadata": {
        "id": "FdKyTd3YLdIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**Machine Learning (ML)** is a subset of artificial intelligence (AI) that enables systems to learn from data, improve their performance, and make decisions or predictions without being explicitly programmed. It involves developing algorithms that can identify patterns in data and use these patterns to make predictions or take actions based on new, unseen data.\n",
        "\n",
        "### Main Components of Machine Learning:\n",
        "\n",
        "1. **Data**:\n",
        "   - Data is the foundation of any machine learning system. It consists of input information that the model uses to learn patterns and make predictions. This data can be structured (e.g., spreadsheets, databases) or unstructured (e.g., images, text).\n",
        "\n",
        "2. **Model**:\n",
        "   - A model in machine learning is a mathematical representation or function that processes the input data to make predictions. The model is \"trained\" using the data, and it learns patterns or relationships in the data.\n",
        "\n",
        "3. **Algorithm**:\n",
        "   - Algorithms are the methods or procedures used to learn from the data and adjust the model. These are the rules and steps that govern how the model is trained. Examples of algorithms include decision trees, linear regression, and neural networks.\n",
        "\n",
        "4. **Training**:\n",
        "   - Training is the process of feeding data into a machine learning algorithm to learn the relationships and patterns in the data. The model is adjusted iteratively to improve accuracy by minimizing the error or loss.\n",
        "\n",
        "5. **Loss Function**:\n",
        "   - A loss function measures how well the model's predictions align with the actual results. The objective of the training process is to minimize the loss function, thereby improving the model’s accuracy.\n",
        "\n",
        "6. **Optimization**:\n",
        "   - Optimization refers to the techniques used to adjust the model’s parameters (weights, biases, etc.) during training to minimize the loss function and improve the model’s performance. Common optimization algorithms include gradient descent.\n",
        "\n",
        "7. **Evaluation**:\n",
        "   - Once the model is trained, it's evaluated using test data (data it has never seen before) to assess its performance. Metrics like accuracy, precision, recall, F1 score, and mean squared error (MSE) are commonly used to evaluate models.\n",
        "\n",
        "8. **Inference**:\n",
        "   - After training and evaluation, the model is deployed to make predictions on new, unseen data. This is known as inference or prediction, where the trained model is used to solve real-world problems.\n",
        "\n",
        "### Types of Machine Learning:\n",
        "- **Supervised Learning**: The model is trained on labeled data (data with known outcomes).\n",
        "- **Unsupervised Learning**: The model works with unlabeled data and tries to find hidden patterns or structures.\n",
        "- **Reinforcement Learning**: The model learns by interacting with an environment and receiving feedback (rewards or penalties) to maximize its performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "NVWFXDD9LoIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does loss value help in determining whether the model is good or not?\n",
        "The **loss value** (or loss function) plays a crucial role in determining whether a machine learning model is performing well or not. It measures how far off the model's predictions are from the actual target values. Essentially, the loss function gives you a quantitative measure of how \"wrong\" the model's predictions are.\n",
        "\n",
        "Here's how the loss value helps in evaluating the model:\n",
        "\n",
        "### 1. **Model Training**:\n",
        "   - During the training process, the model tries to minimize the loss value. The goal is to adjust the model's parameters (weights and biases) to reduce this value over time. A lower loss means the model's predictions are closer to the actual values.\n",
        "   - If the loss value is high, it indicates that the model is performing poorly, and adjustments are needed to improve its accuracy.\n",
        "\n",
        "### 2. **Guiding Optimization**:\n",
        "   - The loss value is used by optimization algorithms (such as gradient descent) to guide the model's parameter updates. If the loss is high, the algorithm will make large adjustments to the model. If the loss is low, the adjustments will be smaller.\n",
        "   - This process continues iteratively until the loss stabilizes at a minimum value, indicating that the model has learned the best set of parameters for the given data.\n",
        "\n",
        "### 3. **Evaluating Model Performance**:\n",
        "   - A **low loss value** means the model is performing well, accurately predicting outcomes or making decisions based on the data.\n",
        "   - A **high loss value** suggests that the model is not capturing the patterns in the data correctly, and it may need more training, more data, or a different model architecture.\n",
        "\n",
        "### 4. **Overfitting vs. Underfitting**:\n",
        "   - **Overfitting**: If the loss is very low on the training data but high on the test data, the model might be overfitting. This means the model is memorizing the training data rather than generalizing to unseen data.\n",
        "   - **Underfitting**: If the loss is high on both the training and test data, the model might be underfitting. This means the model is too simple to capture the underlying patterns in the data.\n",
        "\n",
        "### 5. **Interpreting Loss Values**:\n",
        "   - In general, the **smaller the loss value**, the better the model is. However, it's also important to look at the loss relative to the specific problem and dataset:\n",
        "     - **Mean Squared Error (MSE)**, **Cross-Entropy Loss**, **Hinge Loss**, and other loss functions each have different ranges and meanings.\n",
        "     - The **rate of decrease** in loss over time also matters. If the loss decreases too slowly or plateaus at a high value, the model might need tuning.\n",
        "\n",
        "### 6. **Practical Example**:\n",
        "   - Imagine you're training a regression model to predict house prices. If the model's predictions are far from the actual prices (high loss), it indicates the model is not good at estimating the prices. The goal would be to minimize the loss function (e.g., Mean Squared Error) so that the predictions are as close to the actual prices as possible.\n",
        "\n",
        "### In Summary:\n",
        "- **Loss value** helps monitor how well the model is learning from the data.\n",
        "- A **lower loss value** indicates that the model is improving and is likely a good model.\n",
        "- The **loss trend** during training also gives insights into the model's learning process, whether it is converging properly or if it's overfitting/underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "G5avW6QvL8AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "**Continuous and categorical variables** are types of data used in statistical analysis and machine learning, and they differ in the kind of information they represent and how they can be analyzed.\n",
        "\n",
        "### 1. **Continuous Variables**:\n",
        "   - **Definition**: Continuous variables are numerical variables that can take any value within a given range. These variables can be infinitely divided into smaller increments and represent quantities that can be measured.\n",
        "   - **Characteristics**:\n",
        "     - Can take any value within a range (including decimals or fractions).\n",
        "     - Represent quantities like height, weight, temperature, age, salary, and distance.\n",
        "     - Have an infinite number of possible values between two points.\n",
        "     - Can be represented on a scale with a meaningful order and magnitude.\n",
        "   - **Example**:\n",
        "     - **Height**: A person’s height can be 5.5 feet, 5.51 feet, 5.511 feet, etc.\n",
        "     - **Temperature**: A temperature can be 25.5°C, 25.55°C, and so on.\n",
        "   - **In Machine Learning**: Continuous variables are typically used for regression tasks, where the goal is to predict a continuous output.\n",
        "\n",
        "### 2. **Categorical Variables**:\n",
        "   - **Definition**: Categorical variables represent discrete categories or groups. These variables contain values that can be sorted into distinct classes or labels, but they do not have any intrinsic order or magnitude.\n",
        "   - **Characteristics**:\n",
        "     - Represent qualitative data such as types, labels, or categories.\n",
        "     - Can be **nominal** (no specific order) or **ordinal** (have a specific order).\n",
        "     - **Nominal variables**: These have no specific order, just different categories (e.g., colors, brands, gender).\n",
        "     - **Ordinal variables**: These have a meaningful order, but the distance between categories may not be uniform (e.g., rankings, education level).\n",
        "   - **Example**:\n",
        "     - **Nominal**:\n",
        "       - **Gender**: Male, Female, Other (no specific order).\n",
        "       - **City**: New York, London, Tokyo (no inherent order).\n",
        "     - **Ordinal**:\n",
        "       - **Education Level**: High School, Bachelor's, Master's, Ph.D. (there is an inherent order).\n",
        "       - **Customer Satisfaction**: Poor, Fair, Good, Excellent (the order matters, but the difference between levels isn't necessarily consistent).\n",
        "   - **In Machine Learning**: Categorical variables are typically used for classification tasks, where the goal is to assign a category label to an observation.\n",
        "\n",
        "### Key Differences:\n",
        "| **Attribute**        | **Continuous Variables**               | **Categorical Variables**        |\n",
        "|----------------------|----------------------------------------|----------------------------------|\n",
        "| **Type of Data**      | Quantitative (numerical)               | Qualitative (non-numerical)      |\n",
        "| **Possible Values**   | Infinite, real numbers (decimals allowed) | Discrete categories or labels    |\n",
        "| **Order**             | Can be ordered and measured with meaningful distances | Can be ordered (ordinal) or unordered (nominal) |\n",
        "| **Example**           | Height, weight, temperature, age       | Gender, city, education level    |\n",
        "| **Machine Learning Use** | Regression tasks (predicting numerical values) | Classification tasks (predicting labels) |\n",
        "\n"
      ],
      "metadata": {
        "id": "76mrQDhLMkaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we handle categorical variables in Machine Learning? What are the common t echniques?\n",
        "Handling **categorical variables** is a crucial step in machine learning, as most machine learning algorithms require numerical input. The goal is to convert categorical data into a numerical format that the algorithm can process effectively. There are several techniques for handling categorical variables:\n",
        "\n",
        "### Common Techniques for Handling Categorical Variables:\n",
        "\n",
        "1. **Label Encoding**:\n",
        "   - **Description**: This technique assigns a unique integer to each category in a categorical variable. For example, if you have a \"Color\" column with categories [\"Red\", \"Green\", \"Blue\"], Label Encoding will convert them to [0, 1, 2] respectively.\n",
        "   - **Use Case**: Label Encoding is suitable for ordinal categorical variables (i.e., categories with a meaningful order), such as \"Low\", \"Medium\", and \"High\".\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from sklearn.preprocessing import LabelEncoder\n",
        "     le = LabelEncoder()\n",
        "     data['Color'] = le.fit_transform(data['Color'])\n",
        "     ```\n",
        "   - **Limitations**: It can introduce an artificial ordinal relationship if used on nominal categories (e.g., \"Red\" being encoded as 0 and \"Blue\" as 1 may suggest an order that doesn’t exist).\n",
        "\n",
        "2. **One-Hot Encoding**:\n",
        "   - **Description**: One-Hot Encoding creates a new binary column for each category in the original categorical feature. Each column represents whether a specific category is present (1) or absent (0) in that observation.\n",
        "   - **Use Case**: One-Hot Encoding is ideal for **nominal** categorical variables (i.e., categories without a meaningful order) like \"Color\", \"City\", or \"Brand\".\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     import pandas as pd\n",
        "     data = pd.get_dummies(data, columns=['Color'])\n",
        "     ```\n",
        "     Or using `OneHotEncoder` from Scikit-learn:\n",
        "     ```python\n",
        "     from sklearn.preprocessing import OneHotEncoder\n",
        "     encoder = OneHotEncoder(sparse=False)\n",
        "     encoded_data = encoder.fit_transform(data[['Color']])\n",
        "     ```\n",
        "   - **Limitations**: One-Hot Encoding increases the dimensionality of the dataset, which can lead to the **curse of dimensionality** if there are many unique categories (known as \"high cardinality\").\n",
        "\n",
        "3. **Binary Encoding**:\n",
        "   - **Description**: Binary Encoding is a more compact method than One-Hot Encoding. It converts each category into a binary number and then splits the binary number into separate columns. For instance, \"Red\", \"Green\", and \"Blue\" might be encoded as 00, 01, and 10.\n",
        "   - **Use Case**: This is suitable when you have high cardinality in categorical variables.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from category_encoders import BinaryEncoder\n",
        "     encoder = BinaryEncoder(cols=['Color'])\n",
        "     data = encoder.fit_transform(data)\n",
        "     ```\n",
        "   - **Limitations**: While more efficient than One-Hot Encoding for high cardinality, it might not work well with non-ordinal data in some cases.\n",
        "\n",
        "4. **Frequency or Count Encoding**:\n",
        "   - **Description**: Frequency Encoding assigns each category a number based on the frequency (or count) of that category in the dataset. For example, if \"Red\" appears 10 times, \"Blue\" appears 20 times, and \"Green\" appears 5 times, these categories might be encoded as 10, 20, and 5, respectively.\n",
        "   - **Use Case**: Frequency Encoding is useful when you want to retain some information about the popularity or commonness of a category.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     frequency = data['Color'].value_counts()\n",
        "     data['Color_freq'] = data['Color'].map(frequency)\n",
        "     ```\n",
        "   - **Limitations**: It may not capture the full complexity of categorical relationships and could lead to misleading results if the distribution of categories is skewed.\n",
        "\n",
        "5. **Target Encoding (Mean Encoding)**:\n",
        "   - **Description**: Target Encoding replaces each category with the mean of the target variable for that category. For example, in a dataset with a \"Color\" feature and a target variable \"Price\", each unique \"Color\" would be replaced by the average \"Price\" for that color.\n",
        "   - **Use Case**: Target Encoding works well for **high cardinality** variables where One-Hot Encoding would lead to a large number of columns.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from sklearn.model_selection import StratifiedKFold\n",
        "     import category_encoders as ce\n",
        "     encoder = ce.TargetEncoder(cols=['Color'])\n",
        "     data = encoder.fit_transform(data['Color'], data['Price'])\n",
        "     ```\n",
        "   - **Limitations**: This technique can lead to **data leakage** if the target encoding is done without proper validation splits, as it can use information from the target variable in a way that biases the model.\n",
        "\n",
        "6. **Ordinal Encoding**:\n",
        "   - **Description**: Ordinal Encoding is similar to Label Encoding, but it is specifically for **ordinal categorical variables**, where there is a clear order or ranking. Categories are assigned integer values based on their rank.\n",
        "   - **Use Case**: Used when the categorical variable has a meaningful order, such as \"Low\", \"Medium\", \"High\".\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     ordinal_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "     data['Rank'] = data['Rank'].map(ordinal_mapping)\n",
        "     ```\n",
        "   - **Limitations**: It's not suitable for **nominal variables** (i.e., unordered categories).\n",
        "\n",
        "### Choosing the Right Technique:\n",
        "- **One-Hot Encoding** is generally preferred for nominal data with a small number of unique categories.\n",
        "- **Label Encoding** and **Ordinal Encoding** are best suited for ordinal data, where the categories have a natural order.\n",
        "- **Binary Encoding** is useful when dealing with categorical variables that have many unique values (high cardinality) and when One-Hot Encoding leads to too many columns.\n",
        "- **Frequency Encoding** and **Target Encoding** can be useful for high-cardinality features but need to be used with caution to avoid overfitting and data leakage.\n"
      ],
      "metadata": {
        "id": "Qy9URFIMMlWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "**What do you mean by training and testing a dataset?**\n",
        "   - **Training Dataset**: The training dataset is used to train the machine learning model. It contains input features and the corresponding output (target) values. The model learns the patterns and relationships between the input and output during the training process.\n",
        "   - **Testing Dataset**: The testing dataset is used to evaluate the performance of the model after it has been trained. The model makes predictions based on the test data, and its predictions are compared to the actual values to assess its accuracy and generalization ability.\n",
        "\n",
        " **What is `sklearn.preprocessing`?**\n",
        "   - **`sklearn.preprocessing`** is a module in the `scikit-learn` library that provides various functions for transforming and scaling data. It helps prepare your data for machine learning models by performing tasks like encoding categorical variables, scaling numerical features, and normalizing data.\n",
        "\n",
        "**What is a Test Set?**\n",
        "   - A **test set** is a subset of the data that is not used during the training phase. It is used to evaluate how well the trained model generalizes to new, unseen data. The performance on the test set is used to assess the accuracy, precision, recall, and other metrics of the model.\n",
        "\n",
        " **How do we split data for model fitting (training and testing) in Python?**\n",
        "   - In Python, you can use **`train_test_split`** from `scikit-learn` to split your dataset into a training set and a test set. You can specify the size of the split using the `test_size` argument (e.g., 0.2 means 20% test data and 80% training data).\n",
        "   ```python\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "   ```\n",
        "\n",
        "**How do you approach a Machine Learning problem?**\n",
        "   - **Understand the Problem**: Define the problem clearly and understand the goal.\n",
        "   - **Collect Data**: Gather relevant data from reliable sources.\n",
        "   - **Preprocess Data**: Clean the data (handle missing values, outliers, etc.), encode categorical variables, and scale/normalize numerical features.\n",
        "   - **Split the Data**: Divide the dataset into training and test sets.\n",
        "   - **Select a Model**: Choose a suitable model (e.g., regression, classification).\n",
        "   - **Train the Model**: Fit the model using the training data.\n",
        "   - **Evaluate the Model**: Test the model on the test set and evaluate performance.\n",
        "   - **Tune the Model**: Optimize the model using techniques like cross-validation, hyperparameter tuning, etc.\n",
        "\n",
        " **Why do we have to perform EDA before fitting a model to the data?**\n",
        "   - **Exploratory Data Analysis (EDA)** helps to:\n",
        "     - Understand the distribution of the data.\n",
        "     - Identify patterns, trends, and relationships between variables.\n",
        "     - Detect and handle missing data, outliers, and anomalies.\n",
        "     - Visualize the data to inform model selection and preprocessing techniques.\n",
        "   - EDA ensures that the data is ready for modeling and avoids issues during training.\n",
        "\n",
        ". **What is correlation? What does negative correlation mean?**\n",
        "   - **Correlation** measures the relationship between two variables. It shows whether and how strongly pairs of variables are related.\n",
        "     - **Positive Correlation**: As one variable increases, the other also increases.\n",
        "     - **Negative Correlation**: As one variable increases, the other decreases.\n",
        "   - **Example**: Height and weight have a positive correlation (as height increases, weight generally increases), whereas temperature and heating bills have a negative correlation (as temperature increases, heating bills decrease).\n",
        " **How can you find correlation between variables in Python?**\n",
        "   - You can use the **`corr()`** method in pandas to find the correlation between numerical variables.\n",
        "   ```python\n",
        "   import pandas as pd\n",
        "   df = pd.DataFrame(data)\n",
        "   correlation_matrix = df.corr()\n",
        "   ```\n",
        "\n",
        " **What is causation? Explain difference between correlation and causation with an example.**\n",
        "   - **Causation** means that one variable directly causes the change in another variable. It's a cause-and-effect relationship.\n",
        "   - **Difference**:\n",
        "     - **Correlation**: Two variables may move together, but one does not necessarily cause the other to change.\n",
        "     - **Causation**: One variable causes the change in another.\n",
        "   - **Example**: There is a correlation between ice cream sales and drowning incidents during summer, but eating ice cream does not cause drowning. The cause is the warmer weather, which drives both increased ice cream sales and swimming (leading to more drownings).\n",
        " **What is an Optimizer? What are different types of optimizers?**\n",
        "   - An **Optimizer** is an algorithm used to adjust the model's parameters (weights and biases) during training to minimize the loss function.\n",
        "   - **Types of Optimizers**:\n",
        "     - **Gradient Descent**: A basic optimization algorithm that adjusts parameters by moving in the direction of the negative gradient of the loss function.\n",
        "     - **Stochastic Gradient Descent (SGD)**: A variant of gradient descent where model parameters are updated for each training example.\n",
        "     - **Momentum**: Accelerates gradient descent by adding a fraction of the previous update to the current update.\n",
        "     - **Adam**: Combines the benefits of Momentum and RMSprop by maintaining a moving average of both the gradient and its square.\n",
        "     - **Example**:\n",
        "       ```python\n",
        "       from tensorflow.keras.optimizers import Adam\n",
        "       optimizer = Adam(learning_rate=0.001)\n",
        "       ```\n",
        "\n",
        " **What is `sklearn.linear_model`?**\n",
        "   - **`sklearn.linear_model`** is a module in `scikit-learn` that provides linear models for regression and classification tasks. Examples include `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`, etc.\n",
        "\n",
        " **What does `model.fit()` do? What arguments must be given?**\n",
        "   - **`model.fit()`** is used to train the machine learning model using the training data. It adjusts the model's parameters (weights and biases) to minimize the loss function.\n",
        "   - **Arguments**:\n",
        "     - `X_train`: The feature matrix (input data).\n",
        "     - `y_train`: The target variable (output data).\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     model.fit(X_train, y_train)\n",
        "     ```\n",
        "\n",
        " **What does `model.predict()` do? What arguments must be given?**\n",
        "   - **`model.predict()`** is used to make predictions using the trained model on new, unseen data.\n",
        "   - **Arguments**:\n",
        "     - `X_test`: The feature matrix for which we want to make predictions.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     predictions = model.predict(X_test)\n",
        "     ```\n",
        " **What are continuous and categorical variables?**\n",
        "   - **Continuous Variables**: These are numeric variables that can take any value within a range (e.g., height, weight, temperature).\n",
        "   - **Categorical Variables**: These are variables that represent categories or groups. They can be either **nominal** (no order) or **ordinal** (with order) (e.g., gender, color, education level).\n",
        "\n",
        "**What is feature scaling? How does it help in Machine Learning?**\n",
        "   - **Feature Scaling** is the process of standardizing or normalizing the range of independent variables or features in a dataset. It ensures that all features are on a similar scale and prevents features with larger values from dominating the learning process.\n",
        "   - **Techniques**:\n",
        "     - **Standardization (Z-score scaling)**: Centers the data around 0 with a standard deviation of 1.\n",
        "     - **Normalization**: Scales the data to a fixed range (usually [0, 1]).\n",
        "   - **In Python**:\n",
        "     ```python\n",
        "     from sklearn.preprocessing import StandardScaler\n",
        "     scaler = StandardScaler()\n",
        "     scaled_data = scaler.fit_transform(data)\n",
        "     ```\n",
        "\n",
        " **How do we perform scaling in Python?**\n",
        "   - You can use **`StandardScaler`** or **`MinMaxScaler`** from `sklearn.preprocessing` to scale your data.\n",
        "   ```python\n",
        "   from sklearn.preprocessing import MinMaxScaler\n",
        "   scaler = MinMaxScaler()\n",
        "   scaled_data = scaler.fit_transform(data)\n",
        "   ```\n",
        "\n",
        " **What is data encoding?**\n",
        "   - **Data encoding** is the process of converting categorical variables into numerical format so that they can be used in machine learning models.\n",
        "   - Common techniques include **Label Encoding**, **One-Hot Encoding**, and **Binary Encoding**.\n",
        "\n"
      ],
      "metadata": {
        "id": "efVH_O_XNGn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}