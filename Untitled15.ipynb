{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA33ATIOjmsp0sHjHKo7iN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkarnn/PW_EDA_ASSIGNMENT/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UklHNCLymR91"
      },
      "outputs": [],
      "source": [
        "\n",
        "### **Simple Linear Regression**\n",
        "1. **What is Simple Linear Regression?**\n",
        "   - A statistical method that predicts the dependent variable (Y) based on the independent variable (X).\n",
        "   - Establishes a linear relationship between X and Y using the equation \\( Y = mX + c \\).\n",
        "   - It minimizes the difference between observed and predicted values.\n",
        "   - Commonly used in trend analysis, forecasting, and predictive modeling.\n",
        "   - Suitable when thereâ€™s a single independent variable influencing the dependent variable.\n",
        "\n",
        "2. **What are the key assumptions of Simple Linear Regression?**\n",
        "   - **Linearity**: The relationship between the independent and dependent variable is linear.\n",
        "   - **Independence**: Observations are not correlated with each other.\n",
        "   - **Homoscedasticity**: The variance of residuals remains constant for all values of X.\n",
        "   - **Normality**: Residuals (differences between observed and predicted values) are normally distributed.\n",
        "   - **No Autocorrelation**: Residuals should not be correlated.\n",
        "\n",
        "3. **What does the coefficient \\( m \\) represent in the equation \\( Y = mX + c \\)?**\n",
        "   - Represents the slope of the regression line.\n",
        "   - Indicates how much \\( Y \\) changes for a one-unit increase in \\( X \\).\n",
        "   - A positive \\( m \\) means \\( Y \\) increases as \\( X \\) increases; a negative \\( m \\) means \\( Y \\) decreases.\n",
        "   - Measures the strength of the relationship between \\( X \\) and \\( Y \\).\n",
        "   - Critical in understanding the impact of changes in \\( X \\) on \\( Y \\).\n",
        "\n",
        "4. **What does the intercept \\( c \\) represent in the equation \\( Y = mX + c \\)?**\n",
        "   - Represents the value of \\( Y \\) when \\( X = 0 \\).\n",
        "   - Provides a starting point or baseline for the regression line.\n",
        "   - Helps contextualize the relationship between \\( X \\) and \\( Y \\).\n",
        "   - Can be negative, positive, or zero depending on the data.\n",
        "   - Useful when \\( X = 0 \\) is meaningful within the dataset.\n",
        "\n",
        "5. **How do we calculate the slope \\( m \\) in Simple Linear Regression?**\n",
        "   - Formula: \\( m = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} \\), where Cov is covariance and Var is variance.\n",
        "   - Uses the ratio of shared variance to the variance of \\( X \\).\n",
        "   - Involves summing the product of differences between \\( X \\) and \\( Y \\) means.\n",
        "   - Provides the steepness or inclination of the regression line.\n",
        "   - Can be calculated manually or using statistical software.\n",
        "\n",
        "---\n",
        "\n",
        "### **Multiple Linear Regression**\n",
        "6. **What is Multiple Linear Regression?**\n",
        "   - Models the relationship between one dependent variable and multiple independent variables.\n",
        "   - Equation: \\( Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \\).\n",
        "   - Extends Simple Linear Regression to include multiple predictors.\n",
        "   - Allows for the study of complex relationships between variables.\n",
        "   - Widely used in fields like economics, biology, and social sciences.\n",
        "\n",
        "7. **What is the main difference between Simple and Multiple Linear Regression?**\n",
        "   - **Number of predictors**: Simple has one, while Multiple has two or more.\n",
        "   - **Complexity**: Multiple considers the interaction between variables.\n",
        "   - **Equation form**: Simple uses \\( Y = mX + c \\), while Multiple uses \\( Y = b_0 + \\Sigma (b_nX_n) \\).\n",
        "   - **Interpretation**: Multiple requires controlling for the effect of other variables.\n",
        "   - **Use cases**: Simple is for straightforward relationships, while Multiple handles complex, multivariable scenarios.\n",
        "\n",
        "8. **What are the key assumptions of Multiple Linear Regression?**\n",
        "   - **Linearity**: The dependent variable is linearly related to independent variables.\n",
        "   - **Independence of errors**: Residuals are independent of each other.\n",
        "   - **Homoscedasticity**: Constant variance of errors across all levels of predictors.\n",
        "   - **Multicollinearity**: Independent variables are not highly correlated.\n",
        "   - **Normality of residuals**: Ensures valid hypothesis testing.\n",
        "\n",
        "9. **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "   - **Definition**: Heteroscedasticity occurs when the variance of residuals is not constant across all values of predictors.\n",
        "   - **Impact on results**: Leads to inefficient estimates and biased hypothesis testing.\n",
        "   - **Detection**: Identified through residual plots or statistical tests like Breusch-Pagan.\n",
        "   - **Causes**: Skewed data, omitted variables, or outliers.\n",
        "   - **Solution**: Use transformations (e.g., log) or robust standard errors.\n",
        "\n",
        "10. **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "    - Use **Variance Inflation Factor (VIF)** to identify and remove problematic variables.\n",
        "    - **Combine correlated variables** into a single feature using techniques like PCA.\n",
        "    - Add **regularization techniques** like Ridge or Lasso regression.\n",
        "    - **Standardize or normalize data** to reduce the effect of scaling differences.\n",
        "    - Drop unnecessary predictors to simplify the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Polynomial Regression**\n",
        "11. **What is polynomial regression?**\n",
        "    - Extends linear regression by modeling the relationship as a polynomial.\n",
        "    - Useful for capturing non-linear relationships between variables.\n",
        "    - Adds higher-degree terms (e.g., \\( X^2, X^3 \\)) to the equation.\n",
        "    - Allows for a more flexible fit to complex data patterns.\n",
        "    - Commonly used in fields like physics and machine learning.\n",
        "\n",
        "12. **How does polynomial regression differ from linear regression?**\n",
        "    - **Relationship**: Linear fits a straight line; polynomial fits a curve.\n",
        "    - **Equation**: Polynomial includes higher-degree terms (\\( Y = b_0 + b_1X + b_2X^2 + ... \\)).\n",
        "    - **Flexibility**: Polynomial adapts better to complex data.\n",
        "    - **Overfitting risk**: Higher degrees can lead to overfitting.\n",
        "    - **Interpretation**: Polynomial is harder to interpret compared to linear.\n",
        "\n",
        "13. **When is polynomial regression used?**\n",
        "    - When the relationship between variables is non-linear.\n",
        "    - In cases where linear regression fails to capture trends.\n",
        "    - For modeling curved relationships in physical, biological, or economic systems.\n",
        "    - To achieve higher accuracy in predictive models.\n",
        "    - When residual plots show systematic patterns.\n",
        "\n",
        "14. **What is the general equation for polynomial regression?**\n",
        "    - \\( Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n \\).\n",
        "    - \\( b_0 \\): Intercept; \\( b_1, b_2, ..., b_n \\): Coefficients.\n",
        "    - Includes terms up to the desired polynomial degree.\n",
        "    - Accounts for both linear and non-linear components of data.\n",
        "    - Coefficients can be estimated using the least squares method.\n",
        "\n",
        "15. **What are the limitations of polynomial regression?**\n",
        "    - **Overfitting**: High-degree polynomials can overfit the data.\n",
        "    - **Complexity**: Harder to interpret higher-degree terms.\n",
        "    - **Data scaling**: Sensitive to differences in scale among predictors.\n",
        "    - **Extrapolation issues**: Poor performance outside the observed range.\n",
        "    - **Computational cost**: Increases with higher degrees.\n",
        "\n",
        "---\n"
      ]
    }
  ]
}